{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training IBM Model 1 (non-neural lexical translation model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two things to do before we start:\n",
    "1. Point environment variable `COLLECT_ROOT` to the collection root.\n",
    "2. Change directory to the location of installed scripts/binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: COLLECT_ROOT=/Users/yangchen/Desktop/flexneuart_collections/\n"
     ]
    }
   ],
   "source": [
    "%env COLLECT_ROOT= /Users/yangchen/Desktop/flexneuart_collections/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an IBM Model 1 model\n",
    "Here we create a model for the field text_bert_tok. This script requires MGIZA to be compiled (make sure you ran the script install_packages.sh):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install giza first - bash ~/xxx/flexneuart_install_extra.sh ~/Desktop/flexneuart 1 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /Users/yangchen/Desktop/flexneuart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection root: /Users/yangchen/Desktop/flexneuart_collections/\n",
      "==========================================================================\n",
      "GIZA (output) sub-directory:  giza\n",
      "Bitext sub-directory:         bitext\n",
      "Source dir prefix:            /Users/yangchen/Desktop/flexneuart_collections//wikipedia_dpr_nq_sample/derived_data/bitext\n",
      "Target dir prefix:            /Users/yangchen/Desktop/flexneuart_collections//wikipedia_dpr_nq_sample/derived_data/giza\n",
      "Sample probability:           1\n",
      "==========================================================================\n",
      "Dir=/Users/yangchen/Desktop/flexneuart_collections//wikipedia_dpr_nq_sample/derived_data/giza/text_bert_tok.orig\n",
      "Cleaning up: '/Users/yangchen/Desktop/flexneuart_collections//wikipedia_dpr_nq_sample/derived_data/giza/text_bert_tok.orig'\n",
      ".\n",
      "Full target dir: /Users/yangchen/Desktop/flexneuart_collections/wikipedia_dpr_nq_sample/derived_data/giza\n",
      "Symmetrizing 1, max. fertility 9\n",
      "The sampling and filtering script processed 604204 QA pairs and wrote 595471 original and 595471 flipped pairs\n",
      "Soft-linking source/target files\n",
      "Linking done\n",
      "M-giza bin dir: /Users/yangchen/Desktop/flexneuart/mgiza/mgizapp/bin/\n",
      "source -> source\n",
      "target -> target\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!/Users/yangchen/Desktop/flexneuart/giza/create_tran.sh wikipedia_dpr_nq_sample text_bert_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check, inspect training perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /Users/yangchen/Desktop/flexneuart_collections//wikipedia_dpr_nq_sample/derived_data/giza/text_bert_tok.orig/output.perp: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat $COLLECT_ROOT/wikipedia_dpr_nq_sample/derived_data/giza/text_bert_tok.orig/output.perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It further needs to be pruned and converted to a binary format (infrequent tokens need to be filtered out as well). \n",
    "Note that for BERT-tokenized text, which has less than\n",
    "100K unique tokens, the __specified__ maximum number of most frequent words\n",
    "is too high. However, it makes sense for, e.g.,\n",
    "unlemmatized text fields with large vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "Using collection root: /Users/yangchen/Desktop/flexneuart_collections/\n",
      "=========================================================================================================================\n",
      " This script uses (but doesn't modify), e.g. the data created by ./giza/create_tran.sh which is placed in directory:\n",
      "/Users/yangchen/Desktop/flexneuart_collections//wikipedia_dpr_nq_sample/derived_data/giza/text_bert_tok.orig\n",
      "The filtered output is stored in the following directory:\n",
      "/Users/yangchen/Desktop/flexneuart_collections//wikipedia_dpr_nq_sample/derived_data/giza/text_bert_tok\n",
      "=========================================================================================================================\n",
      "Filtering vocabularies : '/Users/yangchen/Desktop/flexneuart_collections//wikipedia_dpr_nq_sample/derived_data/giza/text_bert_tok.orig' -> '/Users/yangchen/Desktop/flexneuart_collections//wikipedia_dpr_nq_sample/derived_data/giza/text_bert_tok'\n",
      "Source vocabulary file: /Users/yangchen/Desktop/flexneuart_collections//wikipedia_dpr_nq_sample/derived_data/giza/text_bert_tok.orig/source.vcb\n",
      "/Users/yangchen/Desktop/flexneuart/giza/filter_tran_table_and_voc.sh: line 106: FilterVocabulary: command not found\n",
      "**************************************\n",
      "* Failed: filter_voc source\n",
      "**************************************\n"
     ]
    }
   ],
   "source": [
    "!min_tran_prob=0.001 ; top_word_qty=1000000 ; echo $min_tran_prob ; top_word_qty=100000 ; \\\n",
    "/Users/yangchen/Desktop/flexneuart/giza/filter_tran_table_and_voc.sh \\\n",
    "    wikipedia_dpr_nq_sample \\\n",
    "    text_bert_tok \\\n",
    "    $min_tran_prob \\\n",
    "    $top_word_qty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flexneuart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
