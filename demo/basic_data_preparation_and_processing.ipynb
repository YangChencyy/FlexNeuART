{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation/downloading/processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we create the root collection directory and point environment variable `COLLECT_ROOT` to this directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/Desktop/flexneuart_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: COLLECT_ROOT=~/Desktop/flexneuart_collections\n"
     ]
    }
   ],
   "source": [
    "%env COLLECT_ROOT=/Users/yangchen/Desktop/flexneuart_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yangchen/Desktop/flexneuart_collections\n"
     ]
    }
   ],
   "source": [
    "!bash -c \"echo $COLLECT_ROOT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook works with a sub-sample of the natural question collection (__Wikipedia DPR__) prepared by [Karpukhin et al.](https://github.com/facebookresearch/DPR). This subset includes all the questions from __Wikipedia DPR__, but only a sample  of passages (about one million). \n",
    "\n",
    "The generation of this subset is briefly described below, but for your convenience we provide an archive with already processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the directory, downloaded and unpack data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yangchen/Desktop/flexneuart_collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangchen/opt/miniconda3/envs/flexneuart/lib/python3.9/site-packages/IPython/core/magics/osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/Users/yangchen/opt/miniconda3/envs/flexneuart/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd ~/Desktop/flexneuart_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-05 13:38:24--  http://boytsov.info/datasets/wikipedia_dpr_nq_sample_conf_2023-01-17.tar.bz2\n",
      "正在解析主机 boytsov.info (boytsov.info)... 69.60.127.165\n",
      "正在连接 boytsov.info (boytsov.info)|69.60.127.165|:80... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：2692 (2.6K) [application/x-bzip2]\n",
      "正在保存至: “wikipedia_dpr_nq_sample_conf_2023-01-17.tar.bz2”\n",
      "\n",
      "wikipedia_dpr_nq_sa 100%[===================>]   2.63K  --.-KB/s  用时 0s        \n",
      "\n",
      "2024-04-05 13:38:24 (52.4 MB/s) - 已保存 “wikipedia_dpr_nq_sample_conf_2023-01-17.tar.bz2” [2692/2692])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget boytsov.info/datasets/wikipedia_dpr_nq_sample_conf_2023-01-17.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-05 13:38:27--  http://boytsov.info/datasets/wikipedia_dpr_nq_sample_models_2021-09-15.tar.bz2\n",
      "正在解析主机 boytsov.info (boytsov.info)... 69.60.127.165\n",
      "正在连接 boytsov.info (boytsov.info)|69.60.127.165|:80... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：414972906 (396M) [application/x-bzip2]\n",
      "正在保存至: “wikipedia_dpr_nq_sample_models_2021-09-15.tar.bz2”\n",
      "\n",
      "wikipedia_dpr_nq_sa 100%[===================>] 395.75M  2.66MB/s  用时 2m 43s    \n",
      "\n",
      "2024-04-05 13:41:10 (2.43 MB/s) - 已保存 “wikipedia_dpr_nq_sample_models_2021-09-15.tar.bz2” [414972906/414972906])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget boytsov.info/datasets/wikipedia_dpr_nq_sample_models_2021-09-15.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-05 13:45:02--  http://boytsov.info/datasets/wikipedia_dpr_nq_sample_data_2021-09-15.tar.bz2\n",
      "正在解析主机 boytsov.info (boytsov.info)... 69.60.127.165\n",
      "正在连接 boytsov.info (boytsov.info)|69.60.127.165|:80... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：2722927168 (2.5G) [application/x-bzip2]\n",
      "正在保存至: “wikipedia_dpr_nq_sample_data_2021-09-15.tar.bz2”\n",
      "\n",
      "wikipedia_dpr_nq_sa 100%[===================>]   2.54G  10.3MB/s  用时 6m 52s    \n",
      "\n",
      "2024-04-05 13:51:54 (6.30 MB/s) - 已保存 “wikipedia_dpr_nq_sample_data_2021-09-15.tar.bz2” [2722927168/2722927168])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget boytsov.info/datasets/wikipedia_dpr_nq_sample_data_2021-09-15.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-05 13:55:19--  http://boytsov.info/datasets/wikipedia_dpr_nq_sample_bitext_2021-09-15.tar.bz2\n",
      "正在解析主机 boytsov.info (boytsov.info)... 69.60.127.165\n",
      "正在连接 boytsov.info (boytsov.info)|69.60.127.165|:80... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：43284754 (41M) [application/x-bzip2]\n",
      "正在保存至: “wikipedia_dpr_nq_sample_bitext_2021-09-15.tar.bz2”\n",
      "\n",
      "wikipedia_dpr_nq_sa 100%[===================>]  41.28M  4.61MB/s  用时 22s       \n",
      "\n",
      "2024-04-05 13:55:42 (1.89 MB/s) - 已保存 “wikipedia_dpr_nq_sample_bitext_2021-09-15.tar.bz2” [43284754/43284754])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget boytsov.info/datasets/wikipedia_dpr_nq_sample_bitext_2021-09-15.tar.bz2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-05 13:56:30--  http://boytsov.info/datasets/wikipedia_dpr_nq_sample_embed_2021-09-15.tar.bz2\n",
      "正在解析主机 boytsov.info (boytsov.info)... 69.60.127.165\n",
      "正在连接 boytsov.info (boytsov.info)|69.60.127.165|:80... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：55835230 (53M) [application/x-bzip2]\n",
      "正在保存至: “wikipedia_dpr_nq_sample_embed_2021-09-15.tar.bz2”\n",
      "\n",
      "wikipedia_dpr_nq_sa 100%[===================>]  53.25M  7.00MB/s  用时 13s       \n",
      "\n",
      "2024-04-05 13:56:43 (4.19 MB/s) - 已保存 “wikipedia_dpr_nq_sample_embed_2021-09-15.tar.bz2” [55835230/55835230])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget boytsov.info/datasets/wikipedia_dpr_nq_sample_embed_2021-09-15.tar.bz2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x wikipedia_dpr_nq_sample/\n",
      "x wikipedia_dpr_nq_sample/input_data/\n",
      "x wikipedia_dpr_nq_sample/input_data/train_fusion/\n",
      "x wikipedia_dpr_nq_sample/input_data/train_fusion/QuestionFields.jsonl\n",
      "x wikipedia_dpr_nq_sample/input_data/train_fusion/qrels.txt\n",
      "x wikipedia_dpr_nq_sample/input_data/train_fusion/QuestionFields.bin\n",
      "x wikipedia_dpr_nq_sample/input_data/dev/\n",
      "x wikipedia_dpr_nq_sample/input_data/dev/QuestionFields.jsonl\n",
      "x wikipedia_dpr_nq_sample/input_data/dev/qrels.txt\n",
      "x wikipedia_dpr_nq_sample/input_data/dev/QuestionFields.bin\n",
      "x wikipedia_dpr_nq_sample/input_data/bitext/\n",
      "x wikipedia_dpr_nq_sample/input_data/bitext/QuestionFields.jsonl\n",
      "x wikipedia_dpr_nq_sample/input_data/bitext/qrels.txt\n",
      "x wikipedia_dpr_nq_sample/input_data/pass_sample/\n",
      "x wikipedia_dpr_nq_sample/input_data/pass_sample/AnswerFields.bin\n",
      "x wikipedia_dpr_nq_sample/input_data/pass_sample/AnswerFields.jsonl.gz\n",
      "x wikipedia_dpr_nq_sample/input_data/dev_official/\n",
      "x wikipedia_dpr_nq_sample/input_data/dev_official/QuestionFields.jsonl\n",
      "x wikipedia_dpr_nq_sample/input_data/dev_official/qrels.txt\n",
      "x wikipedia_dpr_nq_sample/input_data/dev_official/QuestionFields.bin\n"
     ]
    }
   ],
   "source": [
    "!tar jxvf wikipedia_dpr_nq_sample_data_2021-09-15.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x wikipedia_dpr_nq_sample/\n",
      "x wikipedia_dpr_nq_sample/model_conf/\n",
      "x wikipedia_dpr_nq_sample/model_conf/vanilla_bert.json\n",
      "x wikipedia_dpr_nq_sample/model_conf/vanilla_bert_with_scores.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/extractors/\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/extractors/avgembed.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/extractors/cedr8080.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/extractors/bm25_ance_exported_sparse.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/extractors/bm25=text+model1=text_bert_tok+lambda=0.3+probSelfTran=0.35.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/extractors/bm25_ance.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/extractors/bm25_cedr8080.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/extractors/bm25_avgembed.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/extractors/bm25.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/extractors/ance.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/avgembed.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/cedr8080.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/bm25_ance_exported_sparse.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib_ance.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib_bm25_ance.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/bm25_ance.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/lucene.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/bm25_cedr8080.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib/\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_ance_interleaved/\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_ance_interleaved/cand_prov.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_avgembed/\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_avgembed/fusion_weights\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_avgembed/cand_prov.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib/ance/\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib/ance/fusion_weights\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib/ance/cand_prov.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_ance/\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_ance/fusion_weights\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_ance/cand_prov.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/bm25_avgembed.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib_bm25_ance_interleaved.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/bm25_model1.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/bm25.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/nmslib_bm25_avgembed.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/models/\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/models/bm25_model1.model\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/models/one_feat.model\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/models/bm25_ance.model\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/bm25_test_interm.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc.best/ance.json\n"
     ]
    }
   ],
   "source": [
    "!tar jxvf wikipedia_dpr_nq_sample_conf_2023-01-17.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x wikipedia_dpr_nq_sample/derived_data/bitext/\n",
      "x wikipedia_dpr_nq_sample/derived_data/bitext/answer_text_unlemm\n",
      "x wikipedia_dpr_nq_sample/derived_data/bitext/question_text_bert_tok\n",
      "x wikipedia_dpr_nq_sample/derived_data/bitext/answer_text_bert_tok\n",
      "x wikipedia_dpr_nq_sample/derived_data/bitext/question_text_unlemm\n"
     ]
    }
   ],
   "source": [
    "!tar jxvf wikipedia_dpr_nq_sample_bitext_2021-09-15.tar.bz2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x wikipedia_dpr_nq_sample/derived_data/ir_models/vanilla_bert/model.best\n"
     ]
    }
   ],
   "source": [
    "!tar jxvf wikipedia_dpr_nq_sample_models_2021-09-15.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x wikipedia_dpr_nq_sample/derived_data/embeddings/\n",
      "x wikipedia_dpr_nq_sample/derived_data/embeddings/glove/\n",
      "x wikipedia_dpr_nq_sample/derived_data/embeddings/glove/glove.6B.50d.txt.bz2\n"
     ]
    }
   ],
   "source": [
    "!tar jxvf wikipedia_dpr_nq_sample_embed_2021-09-15.tar.bz2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For all the following experiments we use scripts installed via `flexneuart_install_extra.sh`. They must be called from their respective installation directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yangchen/Desktop/flexneuart\n"
     ]
    }
   ],
   "source": [
    "cd ~/Desktop/flexneuart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carry out a basic sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: report/get_basic_collect_stat.sh\n"
     ]
    }
   ],
   "source": [
    "!report/get_basic_collect_stat.sh wikipedia_dpr_nq_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing in more details : This is for information purposes only because the downloaded data is already pre-processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The download and conversion script can be found in the directory `data_convert/wikipedia_dpr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting passages and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-05 14:13:28--  https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\n",
      "正在解析主机 dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.156.184.22, 108.156.184.129, 108.156.184.78, ...\n",
      "正在连接 dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.156.184.22|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：4694541059 (4.4G) [application/gzip]\n",
      "正在保存至: “psgs_w100.tsv.gz”\n",
      "\n",
      "psgs_w100.tsv.gz    100%[===================>]   4.37G  15.7MB/s  用时 4m 48s    \n",
      "\n",
      "2024-04-05 14:18:17 (15.5 MB/s) - 已保存 “psgs_w100.tsv.gz” [4694541059/4694541059])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!data_convert/wikipedia_dpr/download_dpr_passages.sh $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-05 14:26:47--  https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-train.json.gz\n",
      "正在解析主机 dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.156.184.100, 108.156.184.78, 108.156.184.22, ...\n",
      "正在连接 dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.156.184.100|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：2314892908 (2.2G) [application/gzip]\n",
      "正在保存至: “nq_train.json.gz”\n",
      "\n",
      "nq_train.json.gz    100%[===================>]   2.16G  14.8MB/s  用时 3m 7s     \n",
      "\n",
      "2024-04-05 14:29:54 (11.8 MB/s) - 已保存 “nq_train.json.gz” [2314892908/2314892908])\n",
      "\n",
      "--2024-04-05 14:29:54--  https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz\n",
      "正在解析主机 dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.156.184.78, 108.156.184.22, 108.156.184.129, ...\n",
      "正在连接 dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.156.184.78|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：256239282 (244M) [application/gzip]\n",
      "正在保存至: “nq_dev.json.gz”\n",
      "\n",
      "nq_dev.json.gz      100%[===================>] 244.37M  18.2MB/s  用时 13s       \n",
      "\n",
      "2024-04-05 14:30:08 (18.2 MB/s) - 已保存 “nq_dev.json.gz” [256239282/256239282])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!data_convert/wikipedia_dpr/download_dpr_queries.sh nq $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly split the training set into the new training and development sets. This script also converts the data into FlexNeuART format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection root: /Users/yangchen/Desktop/flexneuart_collections\n",
      "Namespace(seed=0, partitions_names='bitext,train_fusion,dev', partitions_sizes=',5000,2500', src_file='/Users/yangchen/Desktop/flexneuart_collections/wikipedia_dpr_nq_sample/input_raw/nq_train.json.gz', dst_file_pref='/Users/yangchen/Desktop/flexneuart_collections/wikipedia_dpr_nq_sample/input_raw/nq')\n",
      "Reading input files...\n",
      "58880it [01:46, 554.78it/s]\n",
      "Shuffled query IDs using sid 0\n",
      "Final partitions sizes: [('bitext', 51380), ('train_fusion', 5000), ('dev', 2500)]\n",
      "Actually splitting data\n",
      "58880it [09:04, 108.23it/s]\n",
      "Disabled Spacy components:  ['ner', 'parser']\n",
      "BERT-tokenizing input into the field: text_bert_tok\n",
      "tokenizer_config.json: 100%|█████████████████| 48.0/48.0 [00:00<00:00, 18.2kB/s]\n",
      "config.json: 100%|██████████████████████████████| 570/570 [00:00<00:00, 754kB/s]\n",
      "vocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 5.91MB/s]\n",
      "tokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 6.85MB/s]\n",
      "0it [00:00, ?it/s]/Users/yangchen/opt/miniconda3/envs/flexneuart/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "0it [00:00, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yangchen/Desktop/flexneuart/./data_convert/wikipedia_dpr/convert_queries.py\", line 168, in <module>\n",
      "    bi_quest_files[TEXT_BERT_TOKENIZED_NAME].write(query_bert_tok + '\\n')\n",
      "KeyError: 'text_bert_tok'\n"
     ]
    }
   ],
   "source": [
    "!data_convert/wikipedia_dpr/split_and_convert_dpr_queries.sh \\\n",
    "    wikipedia_dpr_nq_sample \\\n",
    "    $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_raw \\\n",
    "    nq \\\n",
    "    -partition_sizes ,5000,2500 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The split & convert script produces outputs of two types:\n",
    "1. The set of questions in JSONL format. These questions are divided into several subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mbitext\u001b[m\u001b[m       \u001b[1m\u001b[36mdev\u001b[m\u001b[m          \u001b[1m\u001b[36mdev_official\u001b[m\u001b[m \u001b[1m\u001b[36mpass_sample\u001b[m\u001b[m  \u001b[1m\u001b[36mtrain_fusion\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bitext` subset and the `train_fusion` subsets are supposed to be used to train models. The difference is that `train_fusion` is a smaller subset that can be used to create fusion models. The `bitext` part can be used to train, e.g., neural models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the queries from the `bitext` set, the conversion script creates parallel data (bitext) where questions are aligned with respective answer-bearing sentences. We create three parallel corpora that correspond to three ways to lemmatize & tokenize input (lemmas and original tokens with stopwords removed and BERT-tokenized text). They are stored in the `derived_data/bitext` subdirectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_text            answer_title_unlemm    question_text_unlemm\n",
      "answer_text_bert_tok   question_text          question_title_unlemm\n",
      "answer_text_unlemm     question_text_bert_tok\n"
     ]
    }
   ],
   "source": [
    "!ls $COLLECT_ROOT/wikipedia_dpr_nq_sample/derived_data/bitext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding document and queries (ANCE, Sentencer BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We already __ship__ data with documents and queries (except for the bitext part) embedded using an [ANCE Wikipedia model](https://github.com/microsoft/ANCE). This is done using the scripts in the `data_convert/biencoder/ance` directory.\n",
    "2. A much more diverse set of embeddings (provided by [Sentence BERT](https://www.sbert.net/)) is available if use the script `data_convert/biencoder/sbert/embed.py`.\n",
    "3. First, one needs to download the models using the script `data_convert/biencoder/ance/download_ance_models.sh`.\n",
    "4. Then, one can embed documents using a command like this one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "data_convert/biencoder/ance/embed.py \\\n",
    "    --input $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_raw/psgs_w100.tsv.gz \\\n",
    "    --output $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_data/pass_sample/AnswerFields.bin \\\n",
    "    --field_name dense  \\\n",
    "    --model_dir <model download directory> \\\n",
    "    --data_type dpr_nq \\\n",
    "    --doc_ids collections/wikipedia_dpr_nq_sample/input_raw/nq_selected_psg_ids.npy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ... and queries using a command like this one (note we specify __the binary field name__):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "data_convert/biencoder/ance/embed.py \\\n",
    "    --input collections/wikipedia_dpr_nq_sample/input_raw/psgs_w100.tsv.gz \\\n",
    "    --output collections/wikipedia_dpr_nq_sample/input_data/pass_sample/AnswerFields.bin \\\n",
    "    --field_name dense  \\\n",
    "    --model_dir <model download directory> \\\n",
    "    --data_type dpr_nq \\\n",
    "    --doc_ids collections/wikipedia_dpr_nq_sample/input_raw/nq_selected_psg_ids.npy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for part in train_fusion dev dev_official ; do \\\n",
    "    data_convert/biencoder/ance/embed.py \\\n",
    "        --input $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_data/$part/QuestionFields.jsonl \\\n",
    "        --output $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_data/$part/QuestionFields.bin \\\n",
    "        --field_name dense  \\\n",
    "        --model_dir <model download directory> \\\n",
    "        --data_type dpr_nq \n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flexneuart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
